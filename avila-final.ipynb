{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_validate, train_test_split, ShuffleSplit, learning_curve, validation_curve\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,\\\n",
    "                            silhouette_score, v_measure_score as vscr\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize, MinMaxScaler, LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture as EM\n",
    "import itertools\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "from IPython.display import display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dill\n",
    "# # # # from dill.settings import settings\n",
    "# dill.dump_session('avila-ntf.db')\n",
    "# # dill_file = open(\"avila-ntf.db\", \"rb\")\n",
    "# dill.load(dill_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avila = pd.read_csv('avila-pro4.csv')\n",
    "print(\"Data has\",len(df_avila),\"rows and\", len(df_avila.columns),\"columns.\")\n",
    "if df_avila.isnull().values.any():\n",
    "    print(\"missing Data\")\n",
    "# df_avila.head()\n",
    "df_avila.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_avila.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df_wilt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(df_avila.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avila.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Counts of Wilt Data Labels\")\n",
    "# sns.countplot(df_wilt['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# names.insert(0, names.pop(names.index('y')))\n",
    "# df_num = df_clean[names[1:]]\n",
    "# x_scaled = min_max_scaler.fit_transform(df_num)\n",
    "# col_names = df_wilt.columns\n",
    "# print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled = pd.DataFrame(x_scaled, columns=names[1:])\n",
    "# df_scaled.head()\n",
    "# rem_names = ['genergy', 'gpuls', 'gdenergy',\n",
    "#        'gdpuls', 'nbumps', 'nbumps2', 'nbumps3', 'nbumps4',\n",
    "#        'nbumps5', 'nbumps6', 'nbumps7', 'nbumps89', 'energy', 'maxenergy',\n",
    "#        'class']\n",
    "\n",
    "\n",
    "# scalar.fit(df_wilt[col_names[1:]])\n",
    "\n",
    "# x_scaled = scalar.transform(df_wilt[col_names[1:]])\n",
    "# x_scaled_test = scalar.transform(df_wilt_test[col_names[1:]])\n",
    "\n",
    "# df_sc = pd.DataFrame(x_scaled, columns=col_names[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_data():\n",
    "\n",
    "\n",
    "    x = np.array(df_avila.values[:,1:])\n",
    "    y = np.array(df_avila.values[:,0])\n",
    "\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avX, avY = get_av_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(avX),np.array(avY), test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(x='class',data=df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.bincount(y_train), np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(df_wilt['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export\n",
    "# df_wilt.to_csv('wilt-tr-pro.csv', index=None)\n",
    "# df_wilt_test.to_csv('wilt-test-pro.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling done only to training data. Testing data is still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded and processed both datasets. We are ready to start the ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src: sklearn\n",
    "def pllc(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 20)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes, scoring='f1')\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    \n",
    "    results ={'sizes':train_sizes,'tr_scores':train_scores_mean, 'val_scores': test_scores_mean, 'title':title}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def tune_hp(estimator, X_train, y_train, title, param_name,param_range, xlabel,xvals=None, cv=5):\n",
    "\n",
    "    \n",
    "            \n",
    "            \n",
    "    train_scores, val_scores = validation_curve(estimator, X_train, y_train, param_name, \n",
    "                                                 param_range, cv=cv, n_jobs=-1, scoring='f1')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    #train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    #test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if xvals is not None:\n",
    "        param_range=xvals\n",
    "            \n",
    "    plt.grid(True)  \n",
    "    plt.plot(param_range, train_scores_mean, 'o-', color = 'g', label='Train Score')\n",
    "    plt.plot(param_range, val_scores_mean, 'o-', color='r', label='Validation Score')\n",
    "    plt.ylabel('F1')\n",
    "    plt.xlabel(xlabel)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_expt(X,title, validate=False, y=None):\n",
    "\n",
    "    kclusters = list(np.arange(1,10,1))\n",
    "    sse, sil_scores, train_times, = [], [], []\n",
    "    v_scores = []\n",
    "\n",
    "    for k in kclusters:\n",
    "        start_time = timeit.default_timer()\n",
    "        km = KMeans(n_clusters=k, n_init=10,random_state=1,n_jobs=-1).fit(X)\n",
    "        sse.append(km.inertia_)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        if k != 1:\n",
    "            sil_scores.append(silhouette_score(X, km.labels_))\n",
    "            if validate:\n",
    "                v_scores.append(vscr(y, km.labels_))\n",
    "            \n",
    "        \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kclusters, sse)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.title('Elbow Plot for KMeans: '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.barplot(x=kclusters[1:],y=sil_scores, color='royalblue')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Avg Silhouette Score')\n",
    "    plt.title('Avg Silhouette Score for KMeans: '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    if validate:\n",
    "        print(\"Validation: Using Training Labels\")\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(kclusters[1:],v_scores)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('No. Clusters')\n",
    "        plt.ylabel('V Score')\n",
    "        plt.title('V Score for KMeans: '+ title)\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_expt(X_train,'Avila Data', validate=True, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_expt(X,title, validate=False, y = None):\n",
    "\n",
    "    klist = list(np.arange(1,10,1))\n",
    "    sil_scores, train_times, aic_scores, bic_scores, v_scores = [], [], [], [], []\n",
    "\n",
    "    for k in klist:\n",
    "        start_time = timeit.default_timer()\n",
    "        em = EM(n_components=k,covariance_type='diag',n_init=1,warm_start=True,random_state=1).fit(X)\n",
    "#         sse.append(em.inertia_)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        aic_scores.append(em.aic(X))\n",
    "        bic_scores.append(em.bic(X))\n",
    "        labels = em.predict(X)\n",
    "        if k != 1:\n",
    "            sil_scores.append(silhouette_score(X,labels))\n",
    "            if validate:\n",
    "                v_scores.append(vscr(y,labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    sns.barplot(x=klist[1:],y=sil_scores, color='royalblue')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Avg Silhouette Score')\n",
    "    plt.title('Avg Silhouette Score for EM: '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     sns.barplot(x=klist[1:],y=dbs_scores)\n",
    "#     plt.grid(True)\n",
    "#     plt.xlabel('No. Clusters')\n",
    "#     plt.ylabel('DBS Score')\n",
    "#     plt.title('DBS Score for EM: '+ title)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     sns.barplot(x=klist[1:],y=chs_scores)\n",
    "#     plt.grid(True)\n",
    "#     plt.xlabel('No. Clusters')\n",
    "#     plt.ylabel('CHS Score')\n",
    "#     plt.title('CHS Score for EM: '+ title)\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.plot(klist, aic_scores, label='AIC')\n",
    "    ax.plot(klist, bic_scores,label='BIC')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('Model Complexity Score')\n",
    "    plt.title('EM Model Complexity: '+ title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if validate:\n",
    "        print(\"Validation:  using Training labels\")\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot()\n",
    "        ax.plot(klist[1:], v_scores)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('No. Distributions')\n",
    "        plt.ylabel('V Score')\n",
    "        plt.title('EM Model V scores: '+ title)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_expt(X_train,'Avila Data', True, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA as ICA, FactorAnalysis, NMF\n",
    "from sklearn.random_projection import GaussianRandomProjection as GRP\n",
    "from itertools import product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA(X,title):\n",
    "    \n",
    "    pca = PCA(random_state=1).fit(X) #for all components\n",
    "    \n",
    "#     xtrain_pca = pca.transform(X)\n",
    "#     projected_pca = pca.inverse_transform(xtrain_pca)\n",
    "#     loss = ((X_train - projected_pca) ** 2).mean()\n",
    "#     print(loss)\n",
    "   \n",
    "    print(pca.explained_variance_)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "#     print(pca.n_components_ )\n",
    "    \n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(list(range(1,len(pca.explained_variance_ratio_)+1)), cum_var, 'r-')\n",
    "    ax1.set_xlabel('Principal Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Cumulative Explained Variance Ratio', color='r')\n",
    "    ax1.tick_params('y', colors='r')\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(list(range(1,len(pca.singular_values_)+1)), pca.singular_values_, 'k-')\n",
    "    ax2.set_ylabel('Eigenvalues')\n",
    "    ax2.tick_params('y', colors='k')\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.title(\"PCA Expl. variance and eigenvalues: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_PCA(X_train,'Avila Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_test(X, m=PCA, title=\"PCA\"):\n",
    "    rg = range(1,X.shape[1]+1)\n",
    "    re_err = []\n",
    "    pca = m(random_state=1)\n",
    "    for i in rg:\n",
    "        pca.set_params(n_components=i)\n",
    "        pca.fit(X) #for all components\n",
    "    #     print(pca.components_)\n",
    "\n",
    "        xtrain_pca = pca.transform(X)\n",
    "        projected_pca = pca.inverse_transform(xtrain_pca)\n",
    "        loss = ((X_train - projected_pca) ** 2).mean()\n",
    "        re_err.append(loss)\n",
    "        print(loss)\n",
    "#         print(loss1)\n",
    "    \n",
    "    \n",
    "    plt.plot(list(rg), re_err, color='royalblue')\n",
    "    plt.xlabel('No. Components')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title(title + ' reconstruction error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_test(X_train, m=PCA, title='PCA Avila' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_ICA(X,title):\n",
    "    feat_len = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    feat_len.append(X.shape[1])\n",
    "    ica = ICA(random_state=1)\n",
    "    kurt = []\n",
    "\n",
    "    for comp in feat_len:\n",
    "        ica.set_params(n_components=comp)\n",
    "        tmp = ica.fit_transform(X)\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp = tmp.kurt(axis=0)\n",
    "        kurt.append(tmp.abs().mean())\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"ICA Kurtosis: \"+ title)\n",
    "    plt.xlabel(\"Independent Components\")\n",
    "    plt.ylabel(\"Avg Kurtosis Across IC\")\n",
    "    plt.plot(feat_len, kurt, 'b-')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ICA(X_train,'Avila Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_test(X_train,m=ICA, title='Avila ICA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "def error_dist_cal(X1,X2):\n",
    "    assert X1.shape[0] == X2.shape[0]\n",
    "    \n",
    "    d1 = pairwise_distances(X1)\n",
    "    d2 = pairwise_distances(X2)\n",
    "    loss = (np.mean((d1-d2)**2))\n",
    "    \n",
    "#     np.corrcoef(d1.ravel(),d2.ravel())[0,1]\n",
    "    return loss\n",
    "\n",
    "def run_GRP(X,title):\n",
    "    \n",
    "    feat_len = list(np.arange(2,(X.shape[1]),1))\n",
    "#     feat_len.append(X.shape[1])\n",
    "    tmp = defaultdict(dict)\n",
    "\n",
    "    for i,comp in product(range(3),feat_len):\n",
    "        rp = GRP(random_state=i, n_components=comp)\n",
    "        tmp[comp][i] = error_dist_cal(rp.fit_transform(X), X)\n",
    "    tmp = pd.DataFrame(tmp).T\n",
    "    mean_recon = tmp.mean(axis=1).tolist()\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(feat_len,mean_recon, 'b-')\n",
    "    ax1.set_xlabel('Random Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Reconstruction Error', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    plt.grid(False)\n",
    "\n",
    "\n",
    "    plt.title(\"Random Components with 3 Restarts: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_GRP(X_train, 'Avila Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_testf(X):\n",
    "    rg = range(2,X.shape[1]+1)\n",
    "    re_err = []\n",
    "    pca = GRP(random_state=1)\n",
    "    for i in rg:\n",
    "        pca.set_params(n_components=i)\n",
    "        pca.fit(X) #for all components\n",
    "    #     print(pca.components_)\n",
    "\n",
    "        xtrain_pca = pca.transform(X)\n",
    "#         projected_pca = pca.inverse_transform(xtrain_pca)\n",
    "        projected_pca = np.dot(xtrain_pca, pca.components_)\n",
    "    \n",
    "    \n",
    "#         loss = ((X_train - projected_pca) ** 2).mean()\n",
    "\n",
    "        loss = ((X_train - projected_pca) ** 2).mean()\n",
    "        re_err.append(loss)\n",
    "        print(loss)\n",
    "    \n",
    "    \n",
    "    sns.barplot(x=list(rg), y=re_err, color='royalblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_testf(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_test(X_train, m=NMF, title='NMF Avila')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=5,random_state=1).fit(X_train)\n",
    "ica1 = ICA(n_components=8,random_state=1).fit(X_train)\n",
    "grp1 = GRP(n_components=6,random_state=1).fit(X_train)\n",
    "nmf1 = NMF(n_components=5,random_state=1).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica2 = ICA(n_components=5,random_state=1).fit(X_train)\n",
    "ica_train2 = ica2.transform(X_train)\n",
    "ica_test2 = ica2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = pca1.transform(X_train)\n",
    "ica_train = ica1.transform(X_train)\n",
    "grp_train = grp1.transform(X_train)\n",
    "nmf_train = nmf1.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying D-reductions to test data (for later use)\n",
    "pca_test = pca1.transform(X_test)\n",
    "ica_test = ica1.transform(X_test)\n",
    "grp_test = grp1.transform(X_test)\n",
    "nmf_test = nmf1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_expt(pca_train,'PCA Avila Data', validate=True, y=y_train)\n",
    "kmeans_expt(ica_train,'ICA Avila Data', validate=True, y=y_train)\n",
    "kmeans_expt(grp_train,'GRP Avila Data', validate=True, y=y_train)\n",
    "kmeans_expt(nmf_train,'NMF Avila Data', validate=True, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_expt(pca_train,'PCA Avila Data', validate=True, y=y_train)\n",
    "EM_expt(ica_train,'ICA Avila Data', validate=True, y=y_train)\n",
    "EM_expt(grp_train,'GRP Avila Data', validate=True, y=y_train)\n",
    "EM_expt(nmf_train,'NMF Avila Data', validate=True, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp =  MLPClassifier(solver='adam',random_state=1, verbose=0, hidden_layer_sizes=(100,200,80,),\n",
    "                          learning_rate_init= 0.01, activation= 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_func(clf,X_train, X_test, y_train, y_test, title):\n",
    "    print(\"Testing for: \" + title)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "\n",
    "    f1 = f1_score(y_test,y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='macro')\n",
    "    recall = recall_score(y_test,y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test,y_pred )\n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, index= ['A', 'E', 'F', 'I'], columns=['A', 'E', 'F', 'I'])\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy))\n",
    "\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision))\n",
    "    print(\"Recall:  \"+\"{:.2f}\".format(recall))\n",
    "\n",
    "\n",
    "    ax = sns.heatmap(df_cm,cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n",
    "    plt.title(title)\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_func(mlp,X_train, X_test, y_train, y_test, \"Avila original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_func(mlp,pca_train, pca_test, y_train, y_test, \"Avila PCA Data\")\n",
    "testing_func(mlp,ica_train, ica_test, y_train, y_test, \"Avila ICA Data\")\n",
    "testing_func(mlp,grp_train, grp_test, y_train, y_test, \"Avila GRP Data\")\n",
    "testing_func(mlp,nmf_train, nmf_test, y_train, y_test, \"Avila NMF Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feat(X_tr, Xt,km_n,em_n):\n",
    "    km = KMeans(n_clusters=km_n,random_state=1,n_jobs=-1).fit(X_tr)\n",
    "    em = EM(n_components=em_n,covariance_type='diag',warm_start=True,random_state=1).fit(X_tr)\n",
    "    if km_n > 2:\n",
    "        km_tr = np.eye(km_n)[km.labels_].astype('int')\n",
    "        km_t = np.eye(km_n)[km.predict(Xt)].astype('int')\n",
    "    else:\n",
    "         \n",
    "        km_tr = km.labels_.reshape(-1,1).astype('int')\n",
    "        km_t = km.predict(Xt).reshape(-1,1).astype('int')\n",
    "        \n",
    "        \n",
    "        \n",
    "    if em_n > 2:\n",
    "        em_tr = np.eye(em_n)[em.predict(X_tr)].astype('int')\n",
    "        em_t = np.eye(em_n)[em.predict(Xt)].astype('int')\n",
    "    else:\n",
    "         \n",
    "        em_tr = em.predict(X_tr).reshape(-1,1).astype('int')\n",
    "        em_t = em.predict(Xt).reshape(-1,1).astype('int')\n",
    "\n",
    "    \n",
    "    km_added_tr = np.hstack((X_tr, km_tr ))\n",
    "    X_tr = np.hstack((km_added_tr, em_tr ))\n",
    "\n",
    "    km_added_t = np.hstack((Xt, km_t ))\n",
    "    X_t = np.hstack((km_added_t, em_t ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_tr, X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p, X_test_p = add_feat(X_train, X_test,4,4)\n",
    "pca_p, pca_tp = add_feat(pca_train,pca_test,4,4)\n",
    "ica_p, ica_tp = add_feat(ica_train,ica_test,3,4)\n",
    "grp_p, grp_tp = add_feat(grp_train,grp_test,3,3)\n",
    "nmf_p, nmf_tp = add_feat(nmf_train,nmf_test,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_func(mlp,X_train_p, X_test_p, y_train, y_test, \"original Data + Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_func(mlp,pca_p, pca_tp, y_train, y_test, \"Avila PCA Data + Clusters\")\n",
    "testing_func(mlp,ica_p, ica_tp, y_train, y_test, \"Avila ICA Data + Clusters\")\n",
    "testing_func(mlp,grp_p, grp_tp, y_train, y_test, \"Avila GRP Data + Clusters\")\n",
    "testing_func(mlp,nmf_p, nmf_tp, y_train, y_test, \"Avila NMF Data + Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(temp.iloc[:,4], temp.iloc[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kmeans_expt1(X,title, validate=False, y=None):\n",
    "\n",
    "#     kclusters = list(np.arange(1,10,1))\n",
    "#     sse, sil_scores, train_times, = [], [], []\n",
    "#     v_scores = []\n",
    "\n",
    "# #     for k in kclusters:\n",
    "   \n",
    "#     km = KMeans(n_clusters=4,random_state=1,n_jobs=-1).fit(X)\n",
    "\n",
    "#     temp = pd.DataFrame(km.labels_)\n",
    "#     print(temp.columns)\n",
    "# #     temp\n",
    "#     sns.lmplot(data=temp,x=1,y=1, hue='label', fit_reg=False, legend=True, legend_out=True)\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans_expt1(pca_train,'PCA Avila Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans_expt(pca_train,'PCA Avila Data')\n",
    "# kmeans_expt(ica_train,'ICA Avila Data')\n",
    "# kmeans_expt(grp_train,'GRP Avila Data')\n",
    "# kmeans_expt(nmf_train,'NMF Avila Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
